{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Red Panda \u00b6 Redpanda is a new storage engine, optimized for streaming data, using a thread-per-core architecture focused on delivering stable tail-latencies Context \u00b6 Companies are not happy to run Kafka, and when they have deep knowledge, they struggle with performance Better hardware with more cache, NUMA architecture, faster drive SSD, cloud container and kubernetes. RedPanda scales more easily and delivers consistently low latencies with a much lower hardware footprint, and with data safety. To get strong guarantee in Kafka you need to flush message to disk, and it is too slow. It is possible to loose messages in Kafka as acknowlege is sent before the fsynch to disk is complete. Red Panda is a good fit for companies doing large scale data streaming without data loss and also in the use case of resource consumption reduction: RedPanda helps reducing power consumption so better for the environment. Value Propositions \u00b6 Kafka compatible cluster. C++ engine. Single binary to deploy. No more zookeeper. It uses the Raft consensus algorithm internally to manage broker cluster. Up to 10x lower tail latencies and 6x faster Kafka transactions on fewer resources. Zero data loss by default, highly available and predictable performance at scale Limitless processing of both real-time and historical data through a single API. Redpanda Keeper (RPK) automatically tunes your kernel to yield the optimal settings for your hardware Transform data with our WebAssembly-based engine: 60% of streaming programming is for doing data manipulation, so to avoid data ping-pong that exists with existing streaming platform, move logic like filtering, data normalization, scrubbing, cleaning in the broker or in sidecar close to it. A wasm engine executes the code. Any wasm modules is supported. With cloud offering Automated backups to S3/GCS JVM app tends to have some latency issue while GC (2 to 6s), but with Kafka messaging that will impact throughput too. An asynch write on a file, will put some performance barrier on the OS Kernel, code has to wait the flush to disk to complete. RedPanda uses batch and debouncing the write operations. No page cached is used, so no lock is used on the kernel (file handlers) to save to dick. File space and metadata are preallocated: No synchronization of the file metadata at the linux kernel. Result 4x throughput and 100x latency improvements The autotuner feature is a tool to assess what the best settings and configurations for running the system: the linux kernel is optimized: network interface, Non-volatile Memory express device (NVMe) Single binary includes broker, HTTP proxy, schema registry Native Prometheus and Grafan integration. Support consumer lag metrics. Terraform and ansible templates available Automatic leader and partition rebalancing, EDA positioning \u00b6 Many of the benefits of an event-processing system can be found in a simple pub/sub process or database that collects events from application components and makes them available to other components. Data as a product: you transform your event stream from individual actions into a warehouse of information about everything that happens in your application Kafka counters \u00b6 Kafka\u2019s default configuration may acknowledge writes before fsync. This might allow Kafka to lose messages when nodes fail. Performance \u00b6 uses DMA (Direct Memory Access) for all its disk IO place the data directory (/var/lib/redpanda/data) on an XFS partition in a local NVMe SSD automatically chooses the best setting to drive high throughput traffic to the machine. everage cgroups to isolate the Redpanda processes leverage systemd slices, to strongly prefer evicting other processes before evicting RedPanda process\u2019 memory and to reserve IO quotas and CPU time CPU is configured for predictable latency at all times Tiered Storage \u00b6 Tiered Storage allows you to save storage costs by offloading log segments to cloud storage. You can specify the amount of local storage that you want to provision and configure Tiered Storage to move the rest to Amazon S3 or Google Cloud Storage. Redpanda Tiered Storage works behind the scenes to index where data is offloaded so that it can retrieve the data when you need it. You can enable Tiered Storage for a cluster or for a topic. Remote write is the process that constantly uploads log segments to cloud storage. The process is created for each partition and runs on the leader node of the partition. The Kafka API \u00b6 Kafka API, and Kafka streams are the winner for integrating and processing data streams. Has millions of line of code. Just change the backbone. RedPanda guarantees compatibility. REST request / response is supported with Kafka REST proxy, and RedPanda offers the same capability. With this API you can replace MQ based product. Kafka API had knowledge of the cluster, bootstrap servers, broker lists, partition leader... But per say Kafka Stream is not that efficient as the consume - processing - produce is using network communication, so this transformation can be better be done with WASM. Wasm engine \u00b6 JavaScript allowed developers to turn static content into the immersive web experiences of today, fundamentally changing the web by shipping code to the user\u2019s computer. Wasm empowers the engineer to transform Redpanda, by shipping computational guarantees (code) to the storage engine. It simply inverts the relation of shipping data to compute by shipping code to data. Redpanda Transforms is based on Google V8 engine and runs in sidecar or within broker. There are 2 types of engines inside Redpanda. The first is for synchronous functions that are in the hot request-response path: transformation function is called before writing the data to the disk. The other is for stateful transformations that are always asynchronous. Current GA feature is the async processing, which is using a child topic-partition created with parent topic creation. Record written to the parent topic, but the transformation will work on each replicas and write to the child. Consumers poll from child topic. Transformation function needs to be idempotent. See data transformation with web assembly section and Red Panda article Getting started \u00b6 See Platform to install the product , or docker brew install vectorizedio/tap/redpanda docker run -d --pull = always --name = redpanda-1 --rm \\ -p 8081 :8081 \\ -p 8082 :8082 \\ -p 9092 :9092 \\ -p 9644 :9644 \\ docker.redpanda.com/vectorized/redpanda:latest redpanda start --overprovisioned \\ --smp 1 --memory 1G --reserve-memory 0M --node-id 0 --check = false # Start a shell docker exec -ti redpanda-1 bash # use rpk commands See also the docker compose under studies/redpanda folder. rpk common commands \u00b6 # on macos directly rpk container start # cluster rpk cluster info --brokers .... # Topic rpk topic create twitch_chat --brokers = localhost:9092 # produce text message like kafka-console-producer rpk topic produce twitch_chat --brokers = localhost:9092 # Consume rpk topic consume twitch_chat --brokers = localhost:9092 # Edit cluster config rpk cluster config edit Data transformation with WebAssembly \u00b6 Code is created in javascript or any language supporting wasm generation. rpk wasm generate program-name Then code is pushed to RedPanda and persisted in a compacted topic so only the last version of code is available: rpk wasm deploy --name = \"encrypt-name\" main.js Pacemaker actively listens for the registration of new scripts (.wasm + .js) via rpk wasm deploy and starts a watch on all new incoming data for the registered topics. The pacemaker is critical for crash recovery. In essence the pacemaker keeps a map of Wasm scripts to offset state. Stateful transforms execute at the lowest level of the storage engine - the partition - and inherit the same partition scalability of the parent stream. At any point in time we can say exactly what scripts are running in the system, on what computers, for what input topics, and we can revoke them and inspect them at runtime. Compendium Podcast - simplify your streaming data workloads with Red Panda. Alex Gallego present this pitch to the IBM Consulting Performance summit 2020 Helm chart Alpaca Launches Next-Gen Order Management System That Makes Order Processing 100x Faster Code generation in Redpanda Infoworld Review: Redpanda gives Kafka a run for its money The Jepsen Redpanda report Terraform deployment done by IBM Data transformation with web assembly section Red Panda article on WASM architecture ]","title":"Home"},{"location":"#red-panda","text":"Redpanda is a new storage engine, optimized for streaming data, using a thread-per-core architecture focused on delivering stable tail-latencies","title":"Red Panda"},{"location":"#context","text":"Companies are not happy to run Kafka, and when they have deep knowledge, they struggle with performance Better hardware with more cache, NUMA architecture, faster drive SSD, cloud container and kubernetes. RedPanda scales more easily and delivers consistently low latencies with a much lower hardware footprint, and with data safety. To get strong guarantee in Kafka you need to flush message to disk, and it is too slow. It is possible to loose messages in Kafka as acknowlege is sent before the fsynch to disk is complete. Red Panda is a good fit for companies doing large scale data streaming without data loss and also in the use case of resource consumption reduction: RedPanda helps reducing power consumption so better for the environment.","title":"Context"},{"location":"#value-propositions","text":"Kafka compatible cluster. C++ engine. Single binary to deploy. No more zookeeper. It uses the Raft consensus algorithm internally to manage broker cluster. Up to 10x lower tail latencies and 6x faster Kafka transactions on fewer resources. Zero data loss by default, highly available and predictable performance at scale Limitless processing of both real-time and historical data through a single API. Redpanda Keeper (RPK) automatically tunes your kernel to yield the optimal settings for your hardware Transform data with our WebAssembly-based engine: 60% of streaming programming is for doing data manipulation, so to avoid data ping-pong that exists with existing streaming platform, move logic like filtering, data normalization, scrubbing, cleaning in the broker or in sidecar close to it. A wasm engine executes the code. Any wasm modules is supported. With cloud offering Automated backups to S3/GCS JVM app tends to have some latency issue while GC (2 to 6s), but with Kafka messaging that will impact throughput too. An asynch write on a file, will put some performance barrier on the OS Kernel, code has to wait the flush to disk to complete. RedPanda uses batch and debouncing the write operations. No page cached is used, so no lock is used on the kernel (file handlers) to save to dick. File space and metadata are preallocated: No synchronization of the file metadata at the linux kernel. Result 4x throughput and 100x latency improvements The autotuner feature is a tool to assess what the best settings and configurations for running the system: the linux kernel is optimized: network interface, Non-volatile Memory express device (NVMe) Single binary includes broker, HTTP proxy, schema registry Native Prometheus and Grafan integration. Support consumer lag metrics. Terraform and ansible templates available Automatic leader and partition rebalancing,","title":"Value Propositions"},{"location":"#eda-positioning","text":"Many of the benefits of an event-processing system can be found in a simple pub/sub process or database that collects events from application components and makes them available to other components. Data as a product: you transform your event stream from individual actions into a warehouse of information about everything that happens in your application","title":"EDA positioning"},{"location":"#kafka-counters","text":"Kafka\u2019s default configuration may acknowledge writes before fsync. This might allow Kafka to lose messages when nodes fail.","title":"Kafka counters"},{"location":"#performance","text":"uses DMA (Direct Memory Access) for all its disk IO place the data directory (/var/lib/redpanda/data) on an XFS partition in a local NVMe SSD automatically chooses the best setting to drive high throughput traffic to the machine. everage cgroups to isolate the Redpanda processes leverage systemd slices, to strongly prefer evicting other processes before evicting RedPanda process\u2019 memory and to reserve IO quotas and CPU time CPU is configured for predictable latency at all times","title":"Performance"},{"location":"#tiered-storage","text":"Tiered Storage allows you to save storage costs by offloading log segments to cloud storage. You can specify the amount of local storage that you want to provision and configure Tiered Storage to move the rest to Amazon S3 or Google Cloud Storage. Redpanda Tiered Storage works behind the scenes to index where data is offloaded so that it can retrieve the data when you need it. You can enable Tiered Storage for a cluster or for a topic. Remote write is the process that constantly uploads log segments to cloud storage. The process is created for each partition and runs on the leader node of the partition.","title":"Tiered Storage"},{"location":"#the-kafka-api","text":"Kafka API, and Kafka streams are the winner for integrating and processing data streams. Has millions of line of code. Just change the backbone. RedPanda guarantees compatibility. REST request / response is supported with Kafka REST proxy, and RedPanda offers the same capability. With this API you can replace MQ based product. Kafka API had knowledge of the cluster, bootstrap servers, broker lists, partition leader... But per say Kafka Stream is not that efficient as the consume - processing - produce is using network communication, so this transformation can be better be done with WASM.","title":"The Kafka API"},{"location":"#wasm-engine","text":"JavaScript allowed developers to turn static content into the immersive web experiences of today, fundamentally changing the web by shipping code to the user\u2019s computer. Wasm empowers the engineer to transform Redpanda, by shipping computational guarantees (code) to the storage engine. It simply inverts the relation of shipping data to compute by shipping code to data. Redpanda Transforms is based on Google V8 engine and runs in sidecar or within broker. There are 2 types of engines inside Redpanda. The first is for synchronous functions that are in the hot request-response path: transformation function is called before writing the data to the disk. The other is for stateful transformations that are always asynchronous. Current GA feature is the async processing, which is using a child topic-partition created with parent topic creation. Record written to the parent topic, but the transformation will work on each replicas and write to the child. Consumers poll from child topic. Transformation function needs to be idempotent. See data transformation with web assembly section and Red Panda article","title":"Wasm engine"},{"location":"#getting-started","text":"See Platform to install the product , or docker brew install vectorizedio/tap/redpanda docker run -d --pull = always --name = redpanda-1 --rm \\ -p 8081 :8081 \\ -p 8082 :8082 \\ -p 9092 :9092 \\ -p 9644 :9644 \\ docker.redpanda.com/vectorized/redpanda:latest redpanda start --overprovisioned \\ --smp 1 --memory 1G --reserve-memory 0M --node-id 0 --check = false # Start a shell docker exec -ti redpanda-1 bash # use rpk commands See also the docker compose under studies/redpanda folder.","title":"Getting started"},{"location":"#rpk-common-commands","text":"# on macos directly rpk container start # cluster rpk cluster info --brokers .... # Topic rpk topic create twitch_chat --brokers = localhost:9092 # produce text message like kafka-console-producer rpk topic produce twitch_chat --brokers = localhost:9092 # Consume rpk topic consume twitch_chat --brokers = localhost:9092 # Edit cluster config rpk cluster config edit","title":"rpk common commands"},{"location":"#data-transformation-with-webassembly","text":"Code is created in javascript or any language supporting wasm generation. rpk wasm generate program-name Then code is pushed to RedPanda and persisted in a compacted topic so only the last version of code is available: rpk wasm deploy --name = \"encrypt-name\" main.js Pacemaker actively listens for the registration of new scripts (.wasm + .js) via rpk wasm deploy and starts a watch on all new incoming data for the registered topics. The pacemaker is critical for crash recovery. In essence the pacemaker keeps a map of Wasm scripts to offset state. Stateful transforms execute at the lowest level of the storage engine - the partition - and inherit the same partition scalability of the parent stream. At any point in time we can say exactly what scripts are running in the system, on what computers, for what input topics, and we can revoke them and inspect them at runtime. Compendium Podcast - simplify your streaming data workloads with Red Panda. Alex Gallego present this pitch to the IBM Consulting Performance summit 2020 Helm chart Alpaca Launches Next-Gen Order Management System That Makes Order Processing 100x Faster Code generation in Redpanda Infoworld Review: Redpanda gives Kafka a run for its money The Jepsen Redpanda report Terraform deployment done by IBM Data transformation with web assembly section Red Panda article on WASM architecture ]","title":"Data transformation with WebAssembly"},{"location":"architecture/","text":"Architecture \u00b6 It uses the Raft consensus algorithm with is a proven distributed consensus protocol: recall that consensus involves multiple servers agreeing on values. The algorithm is well explained here and can be summarized as: a node has 3 states: Follower, Candidate or Leader all nodes starts as a follower. But if they do not get info from a Leader they can become a candidate candidate requests votes from other nodes candidate becomes the leader if it gets votes from a majority of nodes. Requiring a majority of votes guarantees that only one leader can be elected per term. the leader is getting write/ read operations node uses log to keep command of what to do, it pushes replicas to followers, once a majority of nodes have written the entry, then it commits the change, and then notifies the followers that the entry is committed. The cluster has now come to consensus about the system state. For leader election uses 2 timers: election timeout (150ms to 300ms) is the amount of time a follower waits until becoming a candidate. Once it votes for itself, it sends Request Vote messages to other nodes. Heartbeat timeout, controls how often Append Entry messages are sent to followers. Data received from external clients are carried in the Append Entry messages. Raft can stay consistent in the face of network partitions It supports WASM ( WebAssembly ) engine to do inline message transformation between topics via uploading WASM script. The Kafka Stream pattern of consume - process - produce is using network communication that is not efficient. For fraud tolerance, Red Panda is easier to manage as it support a single fault domain, with just one distributed system protocol, while Kafka with Zookeeper and the in-synch protocol has two fault domains, two consensus protocols, which complexifies the problem investigation and the recovery. Shadow indexing feature helps to upload the append log to long persistence storage like s3, COS with indexing. No need for mirror maker. Ask s3 to do replication between DCs, but index is also copied, so historical access is kept. RedPanda guarantees the access and encapsulates that, still using Kafka API. Super cheap using s3 replication. Use the Seastar framework: supports async programming model via futures and promises. Thread-per-core architecture elinates global locks and minimizes I/O blocking, reducing context switching costs. Direct I/O management done by red panda, uses in memory object cache, save to disk after each batch of records. A portion of the memory can be allocated to historical records and portion for fresh written records. It is C++ implementation. Schema registry \u00b6 Support Confluent Schema registry. Read more Raft consensus algorithm","title":"Architecture"},{"location":"architecture/#architecture","text":"It uses the Raft consensus algorithm with is a proven distributed consensus protocol: recall that consensus involves multiple servers agreeing on values. The algorithm is well explained here and can be summarized as: a node has 3 states: Follower, Candidate or Leader all nodes starts as a follower. But if they do not get info from a Leader they can become a candidate candidate requests votes from other nodes candidate becomes the leader if it gets votes from a majority of nodes. Requiring a majority of votes guarantees that only one leader can be elected per term. the leader is getting write/ read operations node uses log to keep command of what to do, it pushes replicas to followers, once a majority of nodes have written the entry, then it commits the change, and then notifies the followers that the entry is committed. The cluster has now come to consensus about the system state. For leader election uses 2 timers: election timeout (150ms to 300ms) is the amount of time a follower waits until becoming a candidate. Once it votes for itself, it sends Request Vote messages to other nodes. Heartbeat timeout, controls how often Append Entry messages are sent to followers. Data received from external clients are carried in the Append Entry messages. Raft can stay consistent in the face of network partitions It supports WASM ( WebAssembly ) engine to do inline message transformation between topics via uploading WASM script. The Kafka Stream pattern of consume - process - produce is using network communication that is not efficient. For fraud tolerance, Red Panda is easier to manage as it support a single fault domain, with just one distributed system protocol, while Kafka with Zookeeper and the in-synch protocol has two fault domains, two consensus protocols, which complexifies the problem investigation and the recovery. Shadow indexing feature helps to upload the append log to long persistence storage like s3, COS with indexing. No need for mirror maker. Ask s3 to do replication between DCs, but index is also copied, so historical access is kept. RedPanda guarantees the access and encapsulates that, still using Kafka API. Super cheap using s3 replication. Use the Seastar framework: supports async programming model via futures and promises. Thread-per-core architecture elinates global locks and minimizes I/O blocking, reducing context switching costs. Direct I/O management done by red panda, uses in memory object cache, save to disk after each batch of records. A portion of the memory can be allocated to historical records and portion for fresh written records. It is C++ implementation.","title":"Architecture"},{"location":"architecture/#schema-registry","text":"Support Confluent Schema registry. Read more Raft consensus algorithm","title":"Schema registry"},{"location":"k8s/","text":"Kubernetes \u00b6 OpenShift Deploy: \u00b6 See deploy app with Helm 3 : curl -L https://mirror.openshift.com/pub/openshift-v4/clients/helm/latest/helm-darwin-amd64 -o /usr/local/bin/helm chmod +x /usr/local/bin/helm helm version Create redpanda project: oc new-project redpanda Get the Helm Chart and modify some values in the values.yaml git clone git@github.com:vectorizedio/helm-charts.git update values.yaml to modify the security context and specify the storage class serviceAccount : create : true annotations : {} name : \"redpanda-sa\" podSecurityContext : {} securityContext : readOnlyRootFilesystem : true runAsNonRoot : true storage : storageClass : \"ibmc-block-gold\" Install helm install --namespace redpanda redpanda ./redpanda/ The output looks like: NAME: redpanda LAST DEPLOYED: Mon Feb 8 16 :49:18 2021 NAMESPACE: redpanda STATUS: deployed REVISION: 1 NOTES: Congratulations on installing redpanda! The pods will rollout in a few seconds. To check the status: kubectl -n redpanda rollout status -w statefulset/redpanda Try some sample commands, like creating a topic called topic1: oc run -ti --rm --restart = Never \\ --image vectorized/redpanda:latest \\ rpk -- --brokers = redpanda-bootstrap:9092 api topic create topic1 To get the api status: kubectl -n redpanda run -ti --rm --restart = Never \\ --image vectorized/redpanda:latest \\ rpk -- --brokers = redpanda-bootstrap:9092 api status Get services: oc get svc and then expose the bootstrap as route: oc expose svc redpanda-bootstrap","title":"Kubernetes"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/#openshift-deploy","text":"See deploy app with Helm 3 : curl -L https://mirror.openshift.com/pub/openshift-v4/clients/helm/latest/helm-darwin-amd64 -o /usr/local/bin/helm chmod +x /usr/local/bin/helm helm version Create redpanda project: oc new-project redpanda Get the Helm Chart and modify some values in the values.yaml git clone git@github.com:vectorizedio/helm-charts.git update values.yaml to modify the security context and specify the storage class serviceAccount : create : true annotations : {} name : \"redpanda-sa\" podSecurityContext : {} securityContext : readOnlyRootFilesystem : true runAsNonRoot : true storage : storageClass : \"ibmc-block-gold\" Install helm install --namespace redpanda redpanda ./redpanda/ The output looks like: NAME: redpanda LAST DEPLOYED: Mon Feb 8 16 :49:18 2021 NAMESPACE: redpanda STATUS: deployed REVISION: 1 NOTES: Congratulations on installing redpanda! The pods will rollout in a few seconds. To check the status: kubectl -n redpanda rollout status -w statefulset/redpanda Try some sample commands, like creating a topic called topic1: oc run -ti --rm --restart = Never \\ --image vectorized/redpanda:latest \\ rpk -- --brokers = redpanda-bootstrap:9092 api topic create topic1 To get the api status: kubectl -n redpanda run -ti --rm --restart = Never \\ --image vectorized/redpanda:latest \\ rpk -- --brokers = redpanda-bootstrap:9092 api status Get services: oc get svc and then expose the bootstrap as route: oc expose svc redpanda-bootstrap","title":"OpenShift Deploy:"},{"location":"usecases/","text":"Some use cases / demonstrations \u00b6 Pinot integration with RedPanda for flight records \u00b6 Inject flights data to RedPanda then to Pinot. See this article . Start redpanda and Pinot # under rpk-pinot-scenario docker compose up -d The pinot-ex folder is mounted inside the docker container of redpanda and pinot controller. flights-schema.json to define the flights table in Pinot Get the flights data curl -X GET https://raw.githubusercontent.com/systemcraftsman/redpanda-apache-pinot-demo/main/resources/data/flights-data.json > flights-data.json Create topic in redpanda: docker exec -ti redpanda-1 bash rpk topic create flights Create table in Pino docker exec -ti pinot-controller bash ./bin/pinot-admin.sh AddTable -schemaFile /tmp/panda_airlines/flights-schema.json -tableConfigFile /tmp/panda_airlines/flights-table-realtime.json -exec Produce message to the flights topic rpk topic produce flights < /tmp/panda_airlines/flights-data.json Connect to the pinot web console chrome localhost:9000 Execute the following query in the Query Console to verify the connection to Kafka has worked and the table in Pinot is loaded with records from flights topic: select * from flights limit 10 Then execute the following SQL to address the analyst's request: Find the number of flights that occurred in January 2014 that have air time of more than 300 minutes and that are from any airport in the state of California to JFK airport. select count ( * ) from flights where Dest = 'JFK' and AirTime > 300 and OriginStateName = 'California' and Month = 1 and Year = 2014","title":"Use cases"},{"location":"usecases/#some-use-cases-demonstrations","text":"","title":"Some use cases / demonstrations"},{"location":"usecases/#pinot-integration-with-redpanda-for-flight-records","text":"Inject flights data to RedPanda then to Pinot. See this article . Start redpanda and Pinot # under rpk-pinot-scenario docker compose up -d The pinot-ex folder is mounted inside the docker container of redpanda and pinot controller. flights-schema.json to define the flights table in Pinot Get the flights data curl -X GET https://raw.githubusercontent.com/systemcraftsman/redpanda-apache-pinot-demo/main/resources/data/flights-data.json > flights-data.json Create topic in redpanda: docker exec -ti redpanda-1 bash rpk topic create flights Create table in Pino docker exec -ti pinot-controller bash ./bin/pinot-admin.sh AddTable -schemaFile /tmp/panda_airlines/flights-schema.json -tableConfigFile /tmp/panda_airlines/flights-table-realtime.json -exec Produce message to the flights topic rpk topic produce flights < /tmp/panda_airlines/flights-data.json Connect to the pinot web console chrome localhost:9000 Execute the following query in the Query Console to verify the connection to Kafka has worked and the table in Pinot is loaded with records from flights topic: select * from flights limit 10 Then execute the following SQL to address the analyst's request: Find the number of flights that occurred in January 2014 that have air time of more than 300 minutes and that are from any airport in the state of California to JFK airport. select count ( * ) from flights where Dest = 'JFK' and AirTime > 300 and OriginStateName = 'California' and Month = 1 and Year = 2014","title":"Pinot integration with RedPanda for flight records"}]}